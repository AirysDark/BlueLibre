name: AI Autobuilder — Android (android/assembleDebug)

on:
  push:
  pull_request:
  workflow_dispatch:

jobs:
  android:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      # Python for ai_autobuilder.py (HTTP client)
      - uses: actions/setup-python@v5
        with: { python-version: "3.11" }
      - run: pip install requests

      # Java + Android SDK
      - uses: actions/setup-java@v4
        with: { distribution: temurin, java-version: "17" }
      - uses: android-actions/setup-android@v3
      - run: yes | sdkmanager --licenses
      - run: sdkmanager "platform-tools" "platforms;android-34" "build-tools;34.0.0"

      # Build (run from android/ if wrapper exists)
      - name: Build (capture)
        id: build
        run: |
          set -euxo pipefail
          if [ -f "android/gradlew" ]; then
            cd android
            chmod +x gradlew
            CMD="./gradlew assembleDebug --stacktrace"
          else
            CMD="gradle -p android assembleDebug --stacktrace"
          fi
          echo "BUILD_CMD=$CMD" >> "$GITHUB_OUTPUT"
          set +e; bash -lc "$CMD" | tee build.log; EXIT=$?; set -e
          echo "EXIT_CODE=$EXIT" >> "$GITHUB_OUTPUT"
          [ -s build.log ] || echo "(no build output captured)" > build.log
          exit 0
        continue-on-error: true

      # Local fallback LLM (llama.cpp) + TinyLlama
      - name: Build llama.cpp (CMake, no CURL)
        run: |
          git clone --depth=1 https://github.com/ggml-org/llama.cpp
          cd llama.cpp
          cmake -S . -B build -D CMAKE_BUILD_TYPE=Release -DLLAMA_CURL=OFF
          cmake --build build -j
          echo "LLAMA_CPP_BIN=$PWD/build/bin/llama-cli" >> $GITHUB_ENV

      - name: Fetch GGUF model (TinyLlama)
        run: |
          mkdir -p models
          curl -L -o models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
            https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

      # Configure git so autobuilder can commit patches
      - name: Configure git identity
        run: |
          git config --global user.name "ai-autobuilder"
          git config --global user.email "ai-autobuilder@github.local"

      # Run the autobuilder (OpenAI → llama fallback) with strict caps
      - name: Attempt AI auto-fix
        if: always() && steps.build.outputs.EXIT_CODE != '0'
        env:
          PROVIDER: openai
          FALLBACK_PROVIDER: llama
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_MODEL: ${{ vars.OPENAI_MODEL || 'gpt-4o-mini' }}
          MODEL_PATH: models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
          AI_BUILDER_ATTEMPTS: "2"
          BUILD_CMD: ${{ steps.build.outputs.BUILD_CMD }}
          LLAMA_CTX: "4096"
          MAX_PROMPT_TOKENS: "2500"
          AI_LOG_TAIL: "80"
          MAX_FILES_IN_TREE: "60"
          RECENT_DIFF_MAX_CHARS: "2000"
        run: python3 tools/ai_autobuilder.py || true

      # Upload outputs and diffs to inspect
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: android-build-and-ai
          path: |
            build.log
            .pre_ai_fix.patch
            android/**/build/outputs/**/*.apk
            **/build/outputs/**/*.apk
          if-no-files-found: warn
